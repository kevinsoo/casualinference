<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text Analysis on Casual Inference</title>
    <link>/tags/text-analysis/</link>
    <description>Recent content in Text Analysis on Casual Inference</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/text-analysis/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Revisiting The Office: A text analysis</title>
      <link>/2020/02/05/revisiting-the-office-a-text-analysis/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/05/revisiting-the-office-a-text-analysis/</guid>
      <description>I’m casually rewatching The Office as my I need something uninvolved to half-occupy my brain for 20 minutes show. Its been almost seven years since the last episode aired, but I still quote it with my closest friends and family – such was its personal and cultural significance. It’s aged like high school: I look back with fondness at the familiar jokes, but there’s also a sense that it was a time we have outgrown.</description>
    </item>
    
    <item>
      <title>Revisiting The Office: A text analysis</title>
      <link>/2020/02/05/revisiting-the-office-a-text-analysis/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/05/revisiting-the-office-a-text-analysis/</guid>
      <description>I&amp;rsquo;m casually rewatching The Office as my I need something uninvolved to half-occupy my brain for 20 minutes show. Its been almost seven years since the last episode aired, but I still quote it with my closest friends and family &amp;ndash; such was its personal and cultural significance. It&amp;rsquo;s aged like high school: I look back with fondness at the familiar jokes, but there&amp;rsquo;s also a sense that it was a time we have outgrown.</description>
    </item>
    
    <item>
      <title>Revisiting The Office: A text analysis</title>
      <link>/2020/02/05/revisiting-the-office-a-text-analysis/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/05/revisiting-the-office-a-text-analysis/</guid>
      <description>I&amp;rsquo;m casually rewatching The Office as my I need something uninvolved to half-occupy my brain for 20 minutes show. Its been almost seven years since the last episode aired, but I still quote it with my closest friends and family &amp;ndash; such was its personal and cultural significance. It&amp;rsquo;s aged like high school: I look back with fondness at the familiar jokes, but there&amp;rsquo;s also a sense that it was a time we have outgrown.</description>
    </item>
    
    <item>
      <title>Dem Debate 1: Topic modeling</title>
      <link>/2019/07/24/dem-debate-1-topic-modeling/</link>
      <pubDate>Wed, 24 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/24/dem-debate-1-topic-modeling/</guid>
      <description>The second Democratic debate is almost here, so I wanted to follow up the sentiment analysis I performed on the first debate by looking at the topics that were talked about across the two nights.
In natural language processing, a topic model is a statistical model that can be used to infer the underlying meanings (i.e. topics) of utterances based on the words being used. In this post, I use Latent Dirichlet Allocation (LDA) from the topicmodels package to infer what each speech by the candidates is about.</description>
    </item>
    
    <item>
      <title>Dem Debate 1: Sentiment analysis</title>
      <link>/2019/07/02/dem-debate-1-sentiment-analysis/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/07/02/dem-debate-1-sentiment-analysis/</guid>
      <description>The 2020 Democratic race finally got underway after last week’s first debates (even though it feels like the race has been going on forever). I’ve been absorbing plenty of commentary of how the two nights transpired (including a few more data-centric and analytical pieces).
Not forgetting the gif-able moments.
I downloaded the full debate transcripts from NBC News (posted here and here) and cleaned it up a little to play around with.</description>
    </item>
    
    <item>
      <title>Predictably comforting: The writings of Soo Ewe Jin</title>
      <link>/2016/11/22/predictably-comforting-the-writings-of-soo-ewe-jin/</link>
      <pubDate>Tue, 22 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/11/22/predictably-comforting-the-writings-of-soo-ewe-jin/</guid>
      <description>My father passed away on November 17, 2016. His life has been memorialized in countless tributes published in the Malaysian press and on social media. An executive editor at the most widely-read English daily in Malaysia, he had a popular weekly column: Sunday Starters.
As a way of navigating the mourning process, I set out to collect all his writings published in The Star. I’m sure the editors would have given me all his writings if I had asked, but instead I wrote some code to scrape all 366 articles he authored from the website.</description>
    </item>
    
  </channel>
</rss>