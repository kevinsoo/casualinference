---
title: Choosing board games
author: Kevin Soo
date: '2019-05-08'
slug: choosing-board-games
categories:
  - R
tags:
  - pop culture
  - games
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(forcats)
library(ggthemes)

# Start from 1981 because that's when Axis & Allies came out - my introduction
df <- read_csv(file = "files/choosing-board-games/bgg_db.csv") %>%
    mutate(id = row_number())

# Parse 15 most common mechanics
df <- df %>%
    mutate(DiceRolling = str_detect(df$mechanic, "Dice Rolling"),
           HandManagement = str_detect(df$mechanic, "Hand Management"),
           VariablePlayerPowers = str_detect(df$mechanic, "Variable Player Powers"),
           SetCollection = str_detect(df$mechanic, "Set Collection"),
           AreaControl = str_detect(df$mechanic, "Area Control / Area Influence"),
           CardDrafting = str_detect(df$mechanic, "Card Drafting"),
           ModularBoard = str_detect(df$mechanic, "Modular Board"),
           TilePlacement = str_detect(df$mechanic, "Tile Placement"),
           HexAndCounter = str_detect(df$mechanic, "Hex-and-Counter"),
           ActionPointAllowanceSystem = str_detect(df$mechanic, "Action Point Allowance System"),
           CooperativePlay = str_detect(df$mechanic, "Co-operative Play"),
           SimultaneousActionSelection = str_detect(df$mechanic, "Simultaneous Action Selection"),
           AuctionBidding = str_detect(df$mechanic, "Auction/Bidding"),
           AreaMovement = str_detect(df$mechanic, "Area Movement"),
           WorkerPlacement = str_detect(df$mechanic, "Worker Placement"))
```

Board games have been a significant part of my life. Many of my closest friendships were forged through repeated gaming sessions. With my friends from high-school, I progressed from simple gateway games like [*Munchkin*](https://www.worldofmunchkin.com/game/) to more complex ones like [*Power Grid*](http://riograndegames.com/Game/5-Power-Grid). I'm certainly [not alone](https://www.theguardian.com/lifeandstyle/2018/may/12/millennials-drive-board-games-revival) as a passenger on the board game train.

What makes a good game, and relatedly, what should I play next? [BoardGameGeek](https://boardgamegeek.com/) (BGG) is an online community that helps answer that question with reviews and discussions of almost anything that qualifies as a board/card game. This past week, I found a dataset of 4,999 BGG entries (up to January 2018) scraped by someone on [Kaggle](https://www.kaggle.com/mrpantherson/board-game-data).

In the following analyses, I'll look at a few factors that might be related to a game's average BGG scores (called [geek-ratings](https://boardgamegeek.com/wiki/page/ratings)), which range from 1 (awful) to 10 (outstanding).

## Games over time

The following plot displays the ratings of games over time. Some games did not have specific release dates (e.g., [*Backgammon*](https://en.wikipedia.org/wiki/Backgammon) and [*Go*](https://en.wikipedia.org/wiki/Go_(game)), so I visualized games starting from 1981 because that encompassed about 95% of the dataset.

```{r bgg, warning=FALSE, echo=FALSE}
df %>%
    filter(year > 1980) %>%
    mutate(votes = log(num_votes)) %>%
    ggplot(aes(x = year, y = geek_rating, color = votes)) +
    geom_jitter(alpha = .25) +
    scale_color_viridis_c() +
    theme_minimal() +
    stat_smooth(method = "lm", color = "black", formula = "y ~ (x)") +
    labs(x = "Year", y = "Rating", color = "log (# of ratings)", 
         title = "BoardGameGeek ratings over time", subtitle = "From 1981 to January 2018")
```

Over time, more games have been released each year. Crucially, games have been getting slightly better on average over time (as can be seen from the positive slope of the regression line).

I also visualized the number of ratings made on BGG for each game. There is a heavy skew in the number of ratings: most games have < 10,000 ratings, but a handful have > 70,000 ratings ([*Catan*](https://www.catan.com/) is the most popular game, with 77,423 ratings). Thus, I applied a [log-transformation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4120293/) to the number of ratings. 

In general, games with better ratings tend to have more ratings -- the correlation between number of ratings and the geek-rating for a game is moderately strong (*r* = .64) This is likely because good games attract more players, who then give more ratings (that tend to be favorable). Since BGG is a trusted source of information in the community, this leads to compounding popularity for the best and most prominent games.

## Game complexity

<!-- Do ratings depend on the structural features of games? One factor that might be related to a game's rating is the average duration of a game. In the dataset, there is hardly any relationship between average duration and geek-ratings (*r* = -.01). However, in the following figure, plotting this relationship by the maximum number of players that each game can accomodate reveals a more subtle pattern. -->

<!-- ```{r gamestructure, warning=FALSE, echo=FALSE} -->
<!-- df %>% -->
<!--     filter(max_players > 0, -->
<!--            min_players > 0) %>% -->
<!--     mutate(max_players = ifelse(max_players > 4, "> 4", max_players), -->
<!--            max_players = factor(max_players, levels = c("1", "2", "3", "4", "> 4"))) %>% -->
<!--     filter(avg_time <= 300, avg_time > 0) %>% -->
<!--     ggplot(aes(x = avg_time, y = geek_rating, color = max_players)) + -->
<!--     geom_jitter(alpha = .1, width = 2) + -->
<!--     stat_smooth(method = "lm", aes(color = max_players), formula = "y ~ (x)") + -->
<!--     scale_color_viridis_d() + -->
<!--     scale_x_continuous(breaks = c(0, 60, 120, 180, 240, 300)) + -->
<!--     theme_minimal() + -->
<!--     labs(x = "Average duration (min)", y = "Rating", color = "Max players",  -->
<!--          title = "Game ratings by duration and max players") -->
<!-- ``` -->

<!-- For games with only 1, 2, or 3 maximum players, there doesn't appear to be a relationship between the average duration of the game with its rating. In fact, for 2-player games, there is a negative relationship (if you're playing a game with only one other person, longer games can feel pretty exhausting). However, for games with a maximum of 4 or more players, there is a positive relationship. Games with more players are probably more sophisticated, and the games may possess enough complexity that longer games are actually better. -->

BGG provides a [weight](https://boardgamegeek.com/geeklist/200613/weight-depth-vs-complexity-results-and-analysis) for each game to measure its complexity (higher weights = more complex games). The following plot reveals a modest positive relationship between these complexity weights and geek-ratings (*r* = .20). In addition, games with longer average durations also tend to have greater complexity.

```{r weights, warning=FALSE, echo=FALSE}
df %>%
    filter(weight > 0) %>%
    filter(avg_time <= 300, avg_time > 0) %>%
    ggplot(aes(x = weight, y = geek_rating, color = avg_time)) +
    geom_point(alpha = .1) +
    stat_smooth(method = "lm", formula = "y ~ (x)", color = "black") +
    scale_color_viridis_c() +
    theme_minimal() +
    labs(x = "Complexity", y = "Rating", color = "Average\nduration\n(min)", title = "Game ratings by complexity")
```

## Game mechanisms

Each game utilizes a handful of mechanisms. There is an element of subjectivity to how the use of different mechanisms influences a game's quality. For example, 116 games in the dataset use a *voting* mechanism, which I don't particularly enjoy (because such games typically involve social persuasion: a key weakness of mine). On the other hand, my more charismatic and sociable friends tend to enjoy such games.

There are 51 different mechanisms named across all games in the dataset. Some of these appear in only a handful of games (e.g., 3 games use the *singing* mechanic, but I'm not sure what that involves). For simplicity, I'll focus on the 15 most common mechanisms. The following figure plots the use of each mechanism across all games by their ranking. There is technically no limit to the number of mechanisms each game can utilize.

```{r mechanic, warning=FALSE, echo=FALSE}
df %>%
    gather(Mechanic, Present, DiceRolling:WorkerPlacement) %>%
    mutate(Mechanic = factor(Mechanic, levels = rev(c("DiceRolling", "HandManagement", 
                                                      "VariablePlayerPowers", "SetCollection", 
                                                      "AreaControl", "CardDrafting", "ModularBoard", 
                                                      "TilePlacement", "HexAndCounter", 
                                                      "ActionPointAllowanceSystem", "CooperativePlay", 
                                                      "SimultaneousActionSelection", "AuctionBidding", 
                                                      "AreaMovement", "WorkerPlacement")))) %>%
    ggplot(aes(x = rank, y = Mechanic)) +
    geom_raster(aes(fill = Present)) +
    scale_fill_viridis_d(name = NULL, labels = c("No", "Yes")) +
    scale_y_discrete(labels = rev(c("Dice rolling", "Hand management", "Variable player powers", 
                                    "Set collection", "Area control", "Card drafting", 
                                    "Modular board", "Tile placement", "Hex-and-counter",
                                    "Actions using points", "Co-op play", "Simultaneous actions",
                                    "Auction/bidding", "Area movement", "Worker placement"))) +
    theme_minimal() +
    labs(x = "Game rank", title = "Game mechanisms by game rank",
         subtitle = "Data for 15 most common game mechanisms")
```

The plot is admittedly difficult to interpret visually. However, it does reveal that the most common mechanism of *dice rolling* is utilized pretty consistently by games of all ranks. *Hex-and-counter*, the 9<sup>th</sup> most common mechanism, actually tends to be missing from higher-ranked games, and is utilized more in games ranked ~2000 or worse. In contrast, *worker placement*, the 15<sup>th</sup> most common mechanism, is utilized more by higher-ranked games.

## Modeling good games

Putting together the insights from above, I built a regression model to see how well the different features predicted game ratings. I first split the overall dataset into a training set (*n* = 3999) and a testing set (*n* = 1000). Using the training set, I fitted a multiple regression model predicting game ratings from all the predictors. The following plot displays the coefficients of each predictor (with 95% confidence intervals).

```{r train, warning=FALSE, echo=FALSE}
# Standardize variables
df <- df %>%
    mutate(num_votes = (num_votes - mean(num_votes)) / sd(num_votes))

# Recode contrasts
contrasts(df$DiceRolling) <- c(-.5, .5)
contrasts(df$HandManagement) <- c(-.5, .5)
contrasts(df$VariablePlayerPowers) <- c(-.5, .5)
contrasts(df$SetCollection) <- c(-.5, .5)
contrasts(df$AreaControl) <- c(-.5, .5)
contrasts(df$CardDrafting) <- c(-.5, .5)
contrasts(df$ModularBoard) <- c(-.5, .5)
contrasts(df$TilePlacement) <- c(-.5, .5)
contrasts(df$HexAndCounter) <- c(-.5, .5)
contrasts(df$ActionPointAllowanceSystem) <- c(-.5, .5)
contrasts(df$CooperativePlay) <- c(-.5, .5)
contrasts(df$SimultaneousActionSelection) <- c(-.5, .5)
contrasts(df$AuctionBidding) <- c(-.5, .5)
contrasts(df$AreaMovement) <- c(-.5, .5)
contrasts(df$WorkerPlacement) <- c(-.5, .5)

# Split dataset
train <- df %>% sample_frac(.80)
test  <- anti_join(df, train, by = 'id')

fit <- lm(data = train,
          geek_rating ~ year + num_votes + weight +
              DiceRolling + HandManagement + VariablePlayerPowers + SetCollection + 
              AreaControl + CardDrafting + ModularBoard + TilePlacement + HexAndCounter +
              ActionPointAllowanceSystem + CooperativePlay + SimultaneousActionSelection +
              AuctionBidding + AreaMovement + WorkerPlacement)

# Get coefficients
coefs <- tibble(
    Predictor = row.names(data.frame(coef(fit))),
    B = as.numeric(coef(fit)),
    Low = confint(fit)[,1],
    High = confint(fit)[,2],
    Type = c("Average", rep("General", 3), rep("Mechanism", 15))
)

# Plot coefficients
coefs %>%
    mutate(Predictor = c("Average", "Year", "# of ratings", "Weight",
                         "Dice rolling", "Hand management", "Variable player powers", 
                         "Set collection", "Area control", "Card drafting", 
                         "Modular board", "Tile placement", "Hex-and-counter",
                         "Actions using points", "Co-op play", "Simultaneous actions", 
                         "Auction/bidding", "Area movement", "Worker placement")) %>%
    filter(Type != "Average") %>%
    ggplot(aes(x = fct_reorder(Predictor, -B), y = B)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "grey") +
    geom_errorbar(aes(ymin = Low, ymax = High), width = .2) +
    geom_point(aes(fill = Type), size = 3, shape = 21) +
    facet_grid(~ Type, scales = "free_x", space = "free_x") +
    scale_fill_viridis_d() +
    theme_minimal() +
    theme(legend.position = "none", 
          axis.text.x  = element_text(angle=90, vjust=0.5)) +
    labs(title = "Influence of predictors on game ratings",
         subtitle = "Error bars represent 95% CIs",
         x = NULL, y = "B")
```

The figure above shows the influence of each predictor while holding all other predictors constant. The # of ratings a game has is positively related to its rating. Because the scale of this predictor was vastly different from the others (ranging from 62 to 77,423), I standardized it -- the coefficient (*B*) indicates that every increase of 1 standard deviation in the # of ratings corresponds to an expected increase of 0.285 in a game's rating. The weight (i.e. complexity) of a game is also positively related to its rating. For every unit increase in complexity (e.g., comparing a game with weight = 2 vs. 3), the expected rating of the game rises by 0.122 points. Holding other predictors constant, there is no relationship between the release year and a game's rating.

The mechanisms are either "absent" or "present" in a game. The coefficients indicate the expected difference in a game's rating when comparing a game without a particular mechanism vs. a game with that mechanism (holding other predictors constant). From the figure, most of the 15 most common mechanisms have a positive relationship with game ratings -- *worker placement* and *card drafting* are particularly desirable. In contrast, the *hex-and-counter* mechanism is actually related to lower game ratings (*B* < 0).

This model accounted for 49.8% of the variation in game ratings. To validate the model, I used it to generate predictions for the testing set -- based on the states of all the predictors for a particular game, I used the model to make a prediction of that game's rating. This yields model predictions for the testing set that can be compared to the actual game ratings.

```{r test, warning=FALSE, echo=FALSE}
# Predict test set
predictions <- predict(fit, test)
test <- test %>% 
    mutate(prediction = predictions,
           error = geek_rating - prediction)
# cor.test(test$geek_rating, test$prediction)
```

The correlation between the model predictions and the actual game ratings in the test set was moderately strong (*r* = .71, equivalent to the model explaining ~ 50% of the variation in game ratings). This very simple model, which only makes use of 15 of the most common mechanisms, does decently well in showing us what makes a well-rated game.

## Next steps

For simplicity, I left out many variables that could explain even more of the variation in game ratings. For example, the dataset contains information about game publishers and categories relating to the theme and style of the games (e.g., *medieval*, *space exploration*). In addition, a deeper investigation should involve combinations of mechanisms (e.g., *actions using points* may not be a positive feature on its own, but perhaps in combination with *worker placement* it makes for very well-rated games).

*This post is dedicated to the friends I learned to play Power Grid with all those years ago. Except the one who always intentionally blocked my path.*